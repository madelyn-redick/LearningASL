{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/madelyn-redick/LearningASL/blob/cnn/CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torchvision.models import ResNet50_Weights\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "_L3bF2BCjt7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Convolutional Neural Network for ASL sign recognition.\n",
        "Extends pytorch's nn.Module.\n",
        "'''\n",
        "class ASL_CNN(nn.Module):\n",
        "  '''\n",
        "  Initializes the CNN Model.\n",
        "  Architecture:\n",
        "    - Transfer learning from resnet50\n",
        "    - Freeze all layers except the last residual block\n",
        "    - Replace fully connected layer with\n",
        "      - Linear -> ReLU -> Dropout -> Linear\n",
        "    - Output has 24 classes (24 static letter signs)\n",
        "  '''\n",
        "  def __init__(self):\n",
        "    super(ASL_CNN, self).__init__()\n",
        "\n",
        "    # Batch normalization as the first layer for input normalization\n",
        "    self.batch_norm = nn.BatchNorm2d(3)\n",
        "\n",
        "    self._base_model = models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
        "\n",
        "    for param in self._base_model.parameters():\n",
        "      param.requires_grad = False\n",
        "\n",
        "    for param in self._base_model.layer4.parameters():\n",
        "      param.requires_grad = True\n",
        "\n",
        "    num_features = self._base_model.fc.in_features # 2048\n",
        "    self._base_model.fc = nn.Sequential(\n",
        "        nn.Linear(num_features, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.4),\n",
        "        nn.Linear(512, 28) # 28 letters (A-Z, DEL, SPACE)\n",
        "    )\n",
        "\n",
        "  '''\n",
        "    Executes a forward pass of the CNN model.\n",
        "\n",
        "    Parameters\n",
        "    x : Tensor\n",
        "      The input image.\n",
        "\n",
        "    Returns a tensor of the model's prediction.\n",
        "  '''\n",
        "  def forward(self, x):\n",
        "    x = self.batch_norm(x)\n",
        "    return self._base_model(x)\n"
      ],
      "metadata": {
        "id": "-QasfpDkjovw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "  Function to train a model.\n",
        "\n",
        "  Parameters\n",
        "  model : nn.Module\n",
        "    A pytorch neural network model.\n",
        "  loss_fn: (Tensor, Tensor) => Tensor (scalar)\n",
        "    A criterion function to calculate loss given the predictions and labels.\n",
        "  optimizer: nn.optim\n",
        "    A pytorch optimizer.\n",
        "  scheduler: torch.optim.lr_scheduler\n",
        "    A pytorch learning rate scheduler.\n",
        "  dataloader: dict\n",
        "    A dictionary containing the DataLoaders for 'train' and 'validation' phases.\n",
        "  num_epochs: int\n",
        "    The number of epochs to train the model for.\n",
        "\n",
        "  Returns the model with weights updated from the best epoch run.\n",
        "'''\n",
        "def train_model(model, loss_fn, optimizer, scheduler, dataloader, num_epochs=20):\n",
        "  torch.save(model.state_dict(), best_model_params_path)\n",
        "  best_accuracy = 0.0\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    print(f'Epoch {epoch}/{num_epochs - 1}')\n",
        "    print('-' * 10)\n",
        "\n",
        "    for phase in ['train', 'validation']:\n",
        "      if phase == 'train':\n",
        "        model.train()\n",
        "      else:\n",
        "        model.eval()\n",
        "\n",
        "      cumulative_loss = 0.0\n",
        "      cumulative_corrects = 0\n",
        "      dataset_size = len(dataloader[phase].dataset)\n",
        "\n",
        "      for inputs, labels in dataloader[phase]:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with torch.set_grad_enabled(phase == 'train'):\n",
        "          outputs = model(inputs)\n",
        "          _, predictions = torch.max(outputs, 1)\n",
        "          loss = loss_fn(outputs, labels)\n",
        "\n",
        "          if phase == 'train':\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        cumulative_loss += loss.item() * inputs.size(0)\n",
        "        cumulative_corrects += torch.sum(predictions == labels.data)\n",
        "\n",
        "      if phase == 'train':\n",
        "        scheduler.step()\n",
        "\n",
        "      epoch_loss = cumulative_loss / dataset_size\n",
        "      epoch_accuracy = cumulative_corrects.double() / dataset_size\n",
        "\n",
        "      print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_accuracy:.4f}')\n",
        "\n",
        "      if phase == 'validation' and epoch_accuracy > best_accuracy:\n",
        "        best_accuracy = epoch_accuracy\n",
        "        torch.save(model.state_dict(), best_model_params_path)\n",
        "\n",
        "  model.load_state_dict(torch.load(best_model_params_path, weights_only=True))\n",
        "  return model\n"
      ],
      "metadata": {
        "id": "7bm5FwWckH1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8ae5598"
      },
      "source": [
        "'''\n",
        "  Function to evaluate a model.\n",
        "\n",
        "  Parameters\n",
        "  model : nn.Module\n",
        "    A pytorch neural network model.\n",
        "  loss_fn: (Tensor, Tensor) => Tensor (scalar)\n",
        "    A criterion function to calculate loss given the predictions and labels.\n",
        "  test_dataloader: DataLoader\n",
        "    The DataLoader for the test dataset.\n",
        "\n",
        "  Returns the test accuracy and loss.\n",
        "'''\n",
        "def evaluate_model(model, loss_fn, test_dataloader):\n",
        "    model.eval()\n",
        "\n",
        "    cumulative_loss = 0.0\n",
        "    cumulative_corrects = 0\n",
        "    test_dataset_size = len(test_dataloader.dataset)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, predictions = torch.max(outputs, 1)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "\n",
        "            cumulative_loss += loss.item() * inputs.size(0)\n",
        "            cumulative_corrects += torch.sum(predictions == labels.data)\n",
        "\n",
        "    test_loss = cumulative_loss / test_dataset_size\n",
        "    test_accuracy = cumulative_corrects.double() / test_dataset_size\n",
        "\n",
        "    print(f'Test Loss: {test_loss:.4f} Test Accuracy: {test_accuracy:.4f}')\n",
        "    return test_loss, test_accuracy\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cb09e1d"
      },
      "source": [
        "import sys\n",
        "# Add path to Python's list of directories to search for modules (in order to find data_preparation file)\n",
        "sys.path.append('../LearningASL')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89183808"
      },
      "source": [
        "import data_preprocessing\n",
        "\n",
        "letter_train = data_preprocessing.letter_train\n",
        "letter_val = data_preprocessing.letter_val\n",
        "letter_test = data_preprocessing.letter_test\n",
        "\n",
        "# Verify imported data\n",
        "print(f\"Type of letter_train: {type(letter_train)}\")\n",
        "print(f\"Length of letter_train: {len(letter_train)}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameter tuning\n",
        "learning_rate = 1e-5 # small learning rate because we are using transfer learning and do not want to mess up pretrained weights\n",
        "momentum = 0.9\n",
        "lr_gamma = 0.9\n",
        "epochs = 30\n",
        "batch_size = 32"
      ],
      "metadata": {
        "id": "Tpe8LhIpbLjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#GPU/CPU and Path Definition\n",
        "best_model_params_path = './best_cnn_params.pth'\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Create DataLoader instances for each phase in training\n",
        "dataloader = {\n",
        "    'train': DataLoader(letter_train, batch_size=batch_size, shuffle=True, num_workers=4),\n",
        "    'validation': DataLoader(letter_val, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "}\n",
        "\n",
        "# Create DataLoader for test set\n",
        "test_dataloader = DataLoader(letter_test, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "# Model Instance\n",
        "model = ASL_CNN()"
      ],
      "metadata": {
        "id": "Fyu4RYXukJ4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimize only the parameters that are not frozen (AKA requires_grad == True)\n",
        "optimizer = optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate, momentum=momentum)\n",
        "scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=lr_gamma)\n",
        "loss = nn.CrossEntropyLoss()\n",
        "\n",
        "train_model(model=model, loss_fn=loss, optimizer=optimizer, scheduler=scheduler, dataloader=dataloader, num_epochs=epochs)"
      ],
      "metadata": {
        "id": "1k5wGI7ObKhU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5adebf8c"
      },
      "source": [
        "# To evaluate model, first load best model weights:\n",
        "loaded_model = ASL_CNN()\n",
        "loaded_model.load_state_dict(torch.load(best_model_params_path, weights_only=True))\n",
        "loaded_model = loaded_model.to(device)\n",
        "\n",
        "test_loss, test_accuracy = evaluate_model(loaded_model, loss, test_dataloader)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
